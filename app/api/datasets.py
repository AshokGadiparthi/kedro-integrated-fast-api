"""
Datasets API - Phase 3
File upload and dataset management with proper metrics calculation

ENDPOINTS:
POST   /api/datasets/{project_id}          Upload file with schema inference
GET    /api/datasets/details/{id}          Get dataset details
GET    /api/datasets/{id}/preview          Get file preview with actual data
GET    /api/datasets/{id}/quality          Get quality metrics
GET    /api/datasets/{project_id}          List all datasets
DELETE /api/datasets/{id}                  Delete dataset
"""

from fastapi import APIRouter, Depends, HTTPException, Header, File, UploadFile, Query
from sqlalchemy.orm import Session
from typing import Optional
import logging
import io
import json
from datetime import datetime

from app.core.database import get_db
from app.models.models import User, Workspace, Project, Dataset
from app.core.auth import verify_token, extract_token_from_header

logger = logging.getLogger(__name__)
router = APIRouter()

# ============================================================================
# HELPER: Get current user
# ============================================================================

def get_current_user(
    authorization: str = Header(None),
    db: Session = Depends(get_db)
) -> User:
    """Extract and verify user from Authorization header"""
    token = extract_token_from_header(authorization)
    if not token:
        raise HTTPException(status_code=401, detail="Missing authorization")
    
    user_id = verify_token(token)
    if not user_id:
        raise HTTPException(status_code=401, detail="Invalid token")
    
    user = db.query(User).filter(User.id == user_id).first()
    if not user:
        raise HTTPException(status_code=401, detail="User not found")
    
    return user


def verify_project_access(project_id: str, user: User, db: Session) -> Project:
    """Verify user has access to project"""
    project = db.query(Project).filter(Project.id == project_id).first()
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")
    
    workspace = db.query(Workspace).filter(
        Workspace.id == project.workspace_id,
        Workspace.owner_id == user.id
    ).first()
    
    if not workspace:
        raise HTTPException(status_code=403, detail="No access to project")
    
    return project


# ============================================================================
# UPLOAD FILE - WITH PROPER METRICS CALCULATION
# ============================================================================

@router.post("/{project_id}", status_code=201, summary="Upload dataset")
async def upload_dataset(
    project_id: str,
    file: UploadFile = File(...),
    name: str = Query(...),
    description: Optional[str] = Query(None),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Upload dataset file and calculate metrics"""
    
    project = verify_project_access(project_id, current_user, db)
    logger.info(f"üì§ Uploading: {file.filename}")
    
    if not file.filename:
        raise HTTPException(400, "No file selected")
    
    file_ext = file.filename.split('.')[-1].lower()
    if file_ext not in ["csv", "json", "parquet", "xlsx", "xls", "tsv"]:
        raise HTTPException(400, f"Unsupported format: .{file_ext}")
    
    try:
        file_content = await file.read()
        file_size = len(file_content)
        if file_size > 1024 * 1024 * 1024:
            raise HTTPException(400, "File too large (max 1GB)")
        logger.info(f"‚úÖ File read: {file_size / 1024:.1f}KB")
    except Exception as e:
        raise HTTPException(400, f"Error reading file: {str(e)}")
    
    # ============================================================================
    # CALCULATE METRICS - PROPERLY!
    # ============================================================================
    
    schema = []
    row_count = 0
    column_count = 0
    quality_score = 0.0
    missing_values_pct = 0.0
    duplicate_rows_pct = 0.0
    
    try:
        import pandas as pd
        
        logger.info(f"üìä Parsing {file_ext} file...")
        
        # Parse file based on format
        if file_ext == "csv":
            df = pd.read_csv(io.BytesIO(file_content))
        elif file_ext == "tsv":
            df = pd.read_csv(io.BytesIO(file_content), sep="\t")
        elif file_ext == "json":
            df = pd.read_json(io.BytesIO(file_content))
        elif file_ext in ["xlsx", "xls"]:
            df = pd.read_excel(io.BytesIO(file_content))
        elif file_ext == "parquet":
            df = pd.read_parquet(io.BytesIO(file_content))
        else:
            raise ValueError(f"Unsupported format: {file_ext}")
        
        logger.info(f"‚úÖ File parsed successfully")
        
        # Get actual counts
        row_count = len(df)
        column_count = len(df.columns)
        
        logger.info(f"üìà Rows: {row_count}, Columns: {column_count}")
        
        # Calculate missing values percentage
        total_cells = row_count * column_count
        missing_cells = df.isnull().sum().sum()
        missing_values_pct = (missing_cells / total_cells * 100) if total_cells > 0 else 0.0
        
        # Calculate duplicate rows percentage
        duplicate_rows = df.duplicated().sum()
        duplicate_rows_pct = (duplicate_rows / row_count * 100) if row_count > 0 else 0.0
        
        # Overall quality score (0-100)
        completeness = (1 - missing_values_pct / 100) * 100
        uniqueness = (1 - duplicate_rows_pct / 100) * 100
        quality_score = (completeness + uniqueness) / 2
        
        logger.info(f"üìä Missing: {missing_values_pct:.1f}%, Duplicates: {duplicate_rows_pct:.1f}%, Quality: {quality_score:.1f}%")
        
        # Build schema with column details
        for col in df.columns:
            col_type = str(df[col].dtype)
            
            if "int" in col_type:
                simple_type = "integer"
            elif "float" in col_type:
                simple_type = "float"
            elif "object" in col_type:
                simple_type = "string"
            elif "datetime" in col_type or "date" in col_type:
                simple_type = "datetime"
            elif "bool" in col_type:
                simple_type = "boolean"
            else:
                simple_type = "string"
            
            schema.append({
                "name": col,
                "type": simple_type,
                "nullable": bool(df[col].isnull().any())
            })
        
        logger.info(f"‚úÖ Schema built: {len(schema)} columns")
        
    except Exception as e:
        logger.error(f"‚ùå Error processing file: {str(e)}")
        schema = []
        row_count = 0
        column_count = 0
        quality_score = 0.0
        missing_values_pct = 0.0
        duplicate_rows_pct = 0.0
    
    # Create dataset with CALCULATED VALUES
    dataset = Dataset(
        project_id=project_id,
        name=name,
        description=description,
        file_name=file.filename,
        file_format=file_ext,
        file_content=file_content,
        row_count=row_count,  # ‚úÖ REAL VALUE
        column_count=column_count,  # ‚úÖ REAL VALUE
        columns_info=schema,
        schema_inferred=len(schema) > 0,
        schema_confidence=95.0 if schema else 0,
        quality_score=quality_score,  # ‚úÖ REAL VALUE
        missing_values_pct=missing_values_pct,  # ‚úÖ REAL VALUE
        duplicate_rows_pct=duplicate_rows_pct,  # ‚úÖ REAL VALUE
        status="ready"
    )
    
    db.add(dataset)
    db.commit()
    db.refresh(dataset)
    
    logger.info(f"‚úÖ Dataset created: {dataset.id}")
    return {
        "id": dataset.id,
        "name": name,
        "file_name": file.filename,
        "file_size": file_size,
        "row_count": row_count,
        "column_count": column_count,
        "quality_score": quality_score,
        "missing_values_pct": missing_values_pct,
        "duplicate_rows_pct": duplicate_rows_pct,
        "schema": schema,
        "status": "ready",
        "created_at": dataset.created_at.isoformat()
    }


# ============================================================================
# GET DATASET DETAILS - SPECIFIC (MUST BE BEFORE GENERIC /{project_id})
# ============================================================================

@router.get("/details/{dataset_id}", summary="Get dataset details")
def get_dataset_details(
    dataset_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Get dataset details"""
    
    dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()
    if not dataset:
        raise HTTPException(404, "Dataset not found")
    
    project = verify_project_access(dataset.project_id, current_user, db)
    
    return {
        "id": dataset.id,
        "name": dataset.name,
        "file_name": dataset.file_name,
        "row_count": dataset.row_count,
        "column_count": dataset.column_count,
        "quality_score": dataset.quality_score,
        "schema": dataset.columns_info,
        "status": dataset.status,
        "created_at": dataset.created_at.isoformat()
    }


# ============================================================================
# GET DATASET PREVIEW - SPECIFIC (MUST BE BEFORE GENERIC /{project_id})
# ============================================================================

@router.get("/{dataset_id}/preview", summary="Get dataset preview")
def get_dataset_preview(
    dataset_id: str,
    rows: int = Query(100, ge=1, le=1000),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Get preview of dataset file (first N rows)"""
    
    dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()
    if not dataset:
        raise HTTPException(404, "Dataset not found")
    
    project = verify_project_access(dataset.project_id, current_user, db)
    
    try:
        import pandas as pd
        
        if dataset.file_format == "csv":
            df = pd.read_csv(io.BytesIO(dataset.file_content), nrows=rows)
        elif dataset.file_format == "tsv":
            df = pd.read_csv(io.BytesIO(dataset.file_content), sep="\t", nrows=rows)
        elif dataset.file_format == "json":
            df = pd.read_json(io.BytesIO(dataset.file_content))
            df = df.head(rows)
        elif dataset.file_format in ["xlsx", "xls"]:
            df = pd.read_excel(io.BytesIO(dataset.file_content), nrows=rows)
        elif dataset.file_format == "parquet":
            df = pd.read_parquet(io.BytesIO(dataset.file_content))
            df = df.head(rows)
        else:
            raise ValueError(f"Unsupported format: {dataset.file_format}")
        
        data = df.to_dict(orient='records')
        
        return {
            "id": dataset.id,
            "name": dataset.name,
            "rows": len(data),
            "columns": list(df.columns),
            "data": data,
            "total_rows": dataset.row_count,  # ‚úÖ FROM DATABASE
            "total_columns": dataset.column_count  # ‚úÖ FROM DATABASE
        }
    except Exception as e:
        logger.error(f"Error reading dataset: {str(e)}")
        raise HTTPException(500, f"Error reading file: {str(e)}")


# ============================================================================
# GET DATASET QUALITY - SPECIFIC (MUST BE BEFORE GENERIC /{project_id})
# ============================================================================

@router.get("/{dataset_id}/quality", summary="Get dataset quality metrics")
def get_dataset_quality(
    dataset_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Get data quality report"""
    
    dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()
    if not dataset:
        raise HTTPException(404, "Dataset not found")
    
    project = verify_project_access(dataset.project_id, current_user, db)
    
    return {
        "id": dataset.id,
        "name": dataset.name,
        "file_name": dataset.file_name,
        "file_size": len(dataset.file_content) if dataset.file_content else 0,
        "row_count": dataset.row_count,  # ‚úÖ ACTUAL
        "column_count": dataset.column_count,  # ‚úÖ ACTUAL
        "quality_score": dataset.quality_score,  # ‚úÖ ACTUAL
        "missing_values_pct": dataset.missing_values_pct or 0,  # ‚úÖ ACTUAL
        "duplicate_rows_pct": dataset.duplicate_rows_pct or 0,  # ‚úÖ ACTUAL
        "schema": dataset.columns_info,
        "status": dataset.status,
        "updated_at": dataset.updated_at.isoformat()
    }


# ============================================================================
# LIST DATASETS - GENERIC (MUST BE LAST)
# ============================================================================

@router.get("/{project_id}", summary="List datasets")
def list_datasets(
    project_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """List all datasets for a project"""
    
    project = verify_project_access(project_id, current_user, db)
    logger.info(f"üìã Listing datasets for project: {project_id}")
    
    datasets = db.query(Dataset).filter(
        Dataset.project_id == project_id,
        Dataset.status != "archived"
    ).all()
    
    return [
        {
            "id": ds.id,
            "name": ds.name,
            "file_name": ds.file_name,
            "file_format": ds.file_format,
            "file_size": len(ds.file_content) if ds.file_content else 0,
            "row_count": ds.row_count,  # ‚úÖ ACTUAL VALUE
            "column_count": ds.column_count,  # ‚úÖ ACTUAL VALUE
            "quality_score": ds.quality_score,  # ‚úÖ ACTUAL VALUE
            "missing_values_pct": ds.missing_values_pct or 0,
            "duplicate_rows_pct": ds.duplicate_rows_pct or 0,
            "status": ds.status,
            "created_at": ds.created_at.isoformat()
        }
        for ds in datasets
    ]


# ============================================================================
# DELETE DATASET
# ============================================================================

@router.delete("/{dataset_id}", status_code=204, summary="Delete dataset")
def delete_dataset(
    dataset_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Delete dataset (soft delete)"""
    
    dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()
    if not dataset:
        raise HTTPException(404, "Dataset not found")
    
    project = verify_project_access(dataset.project_id, current_user, db)
    
    logger.info(f"üóëÔ∏è  Deleting: {dataset_id}")
    dataset.status = "archived"
    dataset.updated_at = datetime.utcnow()
    db.commit()
    logger.info(f"‚úÖ Archived: {dataset_id}")

